<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

 <title>The NoteBlog of Xiaodong Qi</title>
 <link href="/exploringquantumworld.xml" rel="self"/>
 <link href=""/>
 <updated>2016-07-25T05:03:00+00:00</updated>
 <id></id>
 <author>
   <name>Xiaodong Qi</name>
   <email>i2000s@hotmail.com</email>
 </author>

 
 <entry>
   <title>Put everything on a quantum circuit--part I</title>
   <link href="/2016/07/23/put-everything-on-a-quantum-circuit-part-i.html"/>
   <updated>2016-07-23T00:00:00+00:00</updated>
   <id>h/2016/07/23/put-everything-on-a-quantum-circuit-part-i</id>
   <content type="html">&lt;h2 id=&quot;preface&quot;&gt;Preface&lt;/h2&gt;
&lt;p&gt;This series of notes is initially written for &lt;a href=&quot;https://github.com/amitjamadagni&quot;&gt;Amit Jamadagni&lt;/a&gt; and &lt;a href=&quot;https://github.com/Roger-luo&quot;&gt;Xiuzhe Luo&lt;/a&gt; who have been working on the &lt;a href=&quot;http://juliaquantum.github.io&quot;&gt;JuliaQuantum organization&lt;/a&gt;’s &lt;a href=&quot;https://github.com/JuliaQuantum/QuBase.jl&quot;&gt;Base.jl&lt;/a&gt;, &lt;a href=&quot;https://github.com/JuliaQuantum/QuDynamics.jl&quot;&gt;QuDynamics.jl&lt;/a&gt; and &lt;a href=&quot;https://github.com/JuliaQuantum/QuCmp.jl&quot;&gt;QuCmp.jl&lt;/a&gt; projects individually. Amit has put intensive efforts on framing out the basic &lt;a href=&quot;http://julialang.org&quot;&gt;Julia&lt;/a&gt; libraries on basic quantum type system, the time evolution of quantum systems and the idea of propagators. Xiuzhe as a junior undergraduate physics student just started the QuCmp.jl project to build some fundamental Julia libraries for quantum computing (adiabatic quantum computing, quantum circuit model and others). I try to outline in the notes some theoretical foundations on quantum dynamics (especially for open quantum systems) and its link to simulating quantum computing models (especially on the circuit model) with the hope to conclude the following points:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;To reach out the regime of more complicated scenarios to simulate general quantum systems beyond what Amit have done, it would be great to have the stochastic differential equations and their solvers established.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;To be widely useful and efficient, it may be better to reframe the type system on top of operator and superoperator language&lt;a href=&quot;#fn1&quot; class=&quot;footnoteRef&quot; id=&quot;fnref1&quot;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;. It could be easier to start with the complete positive mapping case where the methods and theory have been well established.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;All of the current projects are depending on each other on different levels, and hence developers should know how JuliaQuantum projects are connected in a big picture and work together. That will make the organization and its projects long-lasting and attractive.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;There are a lot to look forward to in building the JuliaQuantum libraries. Once the operator/superoperator type system established, the stochastic equation solves will be easier to implement; once that is done, quantum metrology, tomography and control which usually involve stochastic processes instead of deterministic time-evolution processes will be possible to simulate using the basic packages, and packages for these applications can be developed; the quantum computing package–especially the universal quantum circuit simulation module–is one aspect of those applications as well as the foundation for designing and controlling future quantum computers, the high-performance and generality of which could attract people to join us to develop more useful tools and devices even after the quantum computing times burst.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wish this series of notes could be helpful in sketching a big and inspiring picture in the readers’ mind, instead of trapping their mind into some niches. It may be helpful for other audience. Please leave your comments if you find it helpful or have other thoughts.&lt;/p&gt;
&lt;h2 id=&quot;a-normal-time-evolution-instance-you-may-have-studied&quot;&gt;A normal time-evolution instance you may have studied&lt;/h2&gt;
&lt;p&gt;Let us begin with a simple example: an atomic ensemble interacts with a quantum bath which could be a magnetic field or ideal single photon sources. The time evolution of the density operator may be described by the following differential map &lt;span class=&quot;math&quot;&gt;\[\begin{equation}\label{eq:drhot}
\hat{\rho}(t+dt) = \hat{M}_0(dt)\hat{\rho}(t)\hat{M}^\dagger_0(dt) +
\sum_{\mu&amp;gt;0} \hat{M}_\mu (dt)\hat{\rho}(t)\hat{M}^\dagger(dt)
\end{equation}\]&lt;/span&gt; where the self-evolution operator in the time slot &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt; is given by &lt;span class=&quot;math&quot;&gt;\(\hat{M}_0(dt)=\hat{\mathbb{1}}-\frac{i}{\hbar}\hat{H}_{eff}dt\)&lt;/span&gt; with the effective Hamiltonian &lt;span class=&quot;math&quot;&gt;\(\hat{H}_{eff}=\hat{H}-\frac{i\hbar}{2}\sum_\mu \hat{L}_\mu^\dagger \hat{L}_\mu\)&lt;/span&gt;. The system Hamiltonian &lt;span class=&quot;math&quot;&gt;\(\hat{H}\)&lt;/span&gt; can be arbitrary yet Hermitian. It defines how the system processes without interacting with environment and its energy spectrum (levels). If we consider a spin-&lt;span class=&quot;math&quot;&gt;\(1/2\)&lt;/span&gt; system in the z-basis, a simple free-processing Hamiltonian could be &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\hat{H} &amp;amp;= \left( \matrix{E_+ &amp;amp; 0 \\ 0 &amp;amp; E_-}\right),
\end{align}\]&lt;/span&gt; where &lt;span class=&quot;math&quot;&gt;\(E_+\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(E_-\)&lt;/span&gt; are the two energy levels. Obviously, it can be rewritten using the Pauli operators.&lt;/p&gt;
&lt;p&gt;The so-called jump operators &lt;span class=&quot;math&quot;&gt;\(\hat{L}_\mu\)&lt;/span&gt; define how the system will evolve in each measurement basis &lt;span class=&quot;math&quot;&gt;\(\mu\)&lt;/span&gt;. A quantum state will be evolved into &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\ket{\Psi(t+dt)} &amp;amp;= \hat{L}_\mu(dt) \ket{\Psi(t)}
\end{align}\]&lt;/span&gt; if only considering the single jump operator &lt;span class=&quot;math&quot;&gt;\(\hat{L}_\mu\)&lt;/span&gt; applied in a small period of time, &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt;. We call &lt;span class=&quot;math&quot;&gt;\(\hat{L}_\mu\)&lt;/span&gt; a jump operator because it projects the quantum state towards some eigenstate of the operator randomly. For example, we could let $ \hat{L}&lt;em&gt;i = \sigma&lt;/em&gt;z $ for a spin-&lt;span class=&quot;math&quot;&gt;\(1/2\)&lt;/span&gt; system, which means &lt;span class=&quot;math&quot;&gt;\(\hat{L}_i\)&lt;/span&gt; will tend to project the system onto one of its eigenstate for spin number &lt;span class=&quot;math&quot;&gt;\(\pm \frac{1}{2}\)&lt;/span&gt; in the z-basis. To give a concrete sense of physics, this type of jump operators could corresponding to a magnetic field interaction–the one as we know in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment&quot;&gt;Stern-Gerlach experiment&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;We can formally define a set of Krause operators &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\hat{M}_\mu &amp;amp;= \hat{L}_\mu \sqrt{dt},\quad \mu=1,\cdots,m.
\end{align}\]&lt;/span&gt; Notice that each &lt;span class=&quot;math&quot;&gt;\(\hat{M}_\mu\)&lt;/span&gt; is on the order of &lt;span class=&quot;math&quot;&gt;\(\sqrt{dt}\)&lt;/span&gt; which makes the Krause operator a second-order effect on the evolution of the system. We define the Krause operators in this way is based on the fact that the measurement operators &lt;span class=&quot;math&quot;&gt;\(\hat{E}_\mu\)&lt;/span&gt; and the probability of finding the output on the &lt;span class=&quot;math&quot;&gt;\(\mu\)&lt;/span&gt; channel &lt;span class=&quot;math&quot;&gt;\(p_\mu\)&lt;/span&gt; should be &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\hat{E}_\mu &amp;amp;= \hat{M}^\dagger_\mu\hat{M}_\mu \\
p_\mu &amp;amp;= \tr\left( \hat{\rho}\hat{E}_\mu\right) \label{eq:jumppmu}
\end{align}\]&lt;/span&gt; which are incremental over &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt; time slots. The post-measurement state can therefore be written as &lt;span class=&quot;math&quot;&gt;\[\begin{align}\label{eq:rhodt}
\hat{\rho}_\mu = \frac{\hat{M}_\mu\hat{\rho}\hat{M}^\dagger_\mu}{p_\mu}.
\end{align}\]&lt;/span&gt; For a pure state &lt;span class=&quot;math&quot;&gt;\(\hat{\rho}=\ketbra{\Psi}{\Psi}\)&lt;/span&gt;&lt;a href=&quot;#fn2&quot; class=&quot;footnoteRef&quot; id=&quot;fnref2&quot;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;, &lt;span class=&quot;math&quot;&gt;\[\begin{align} \label{eq:Psidt}
\ket{\Psi(t+dt)}_\mu = \frac{\hat{M}_\mu\ket{\Psi(t)}}{\sqrt{p_\mu}} = \frac{\hat{L}_\mu\ket{\Psi(t)}}{ \abs{\hat{L}_\mu\ket{\Psi(t)}} }.
\end{align}\]&lt;/span&gt; Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:Psidt}\)&lt;/span&gt; may look more familiar than Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:rhodt}\)&lt;/span&gt; as the former one is commonly used to describe Born’s rules on quantum measurements.&lt;/p&gt;
&lt;p&gt;If we look at Eqs.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:Psidt}\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(\eqref{eq:rhodt}\)&lt;/span&gt; more closely, we will find that both has been used in defining the algorithms of &lt;strong&gt;&lt;em&gt;quantum diffusion Monte Carlo&lt;/em&gt;&lt;/strong&gt; (QDMC) or &lt;strong&gt;&lt;em&gt;quantum wavefunction Monte Carlo&lt;/em&gt;&lt;/strong&gt; (QWFMC) method&lt;a href=&quot;#Dum1992Monte&quot;&gt; [1,2,3,4]&lt;/a&gt;. The &lt;span class=&quot;math&quot;&gt;\(\hat{L}_\mu\)&lt;/span&gt; operator indeed defines the random quantum jumps in the algorithm with a jump probability defined in Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:jumppmu}\)&lt;/span&gt;! Each jump will lead to a new quantum trajectory and &lt;em&gt;diffuse&lt;/em&gt; the quantum states over time.&lt;/p&gt;
&lt;p&gt;Further more, if we rewrite Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:drhot}\)&lt;/span&gt; in the form of an ordinary differential equation, we have obtain &lt;span class=&quot;math&quot;&gt;\[\begin{align} \label{eq:lindblad}
\dd{\rho}{t} &amp;amp;= -\frac{i}{\hbar} \left[ \hat{H},\hat{\rho}\right]-\frac{1}{2}\sum_{\mu=1}^m \left( \hat{L}_\mu^\dagger\hat{L}_\mu\hat{\rho}+\hat{\rho}\hat{L}^\dagger_\mu\hat{L}_\mu-2\hat{L}_\mu\hat{\rho}\hat{L}^\dagger_\mu \right).
\end{align}\]&lt;/span&gt; This is the famous &lt;em&gt;Lindblad&lt;/em&gt; form of the &lt;strong&gt;&lt;em&gt;quantum master equation&lt;/em&gt;&lt;/strong&gt; for an open system! In many textbooks, the Lindblad equation can also be formally simplified by defining a superoperator &lt;span class=&quot;math&quot;&gt;\(\mathcal{D}[\cdot]\)&lt;/span&gt; via &lt;span class=&quot;math&quot;&gt;\[\begin{equation}
\mathcal{D}[\hat{\rho}] = -\frac{i}{\hbar} \left[ \hat{H},\hat{\rho}\right]-\frac{1}{2}\sum_{\mu=1}^m \left( \hat{L}_\mu^\dagger\hat{L}_\mu\hat{\rho}+\hat{\rho}\hat{L}^\dagger_\mu\hat{L}_\mu-2\hat{L}_\mu\hat{\rho}\hat{L}^\dagger_\mu \right)
\end{equation}\]&lt;/span&gt; so that &lt;span class=&quot;math&quot;&gt;\(\dd{\hat{\rho}}{t}=\mathcal{D}[\hat{\rho}]\)&lt;/span&gt; describes a dissipative process in a neat form. In the mean time, the jump operators &lt;span class=&quot;math&quot;&gt;\(\hat{L}_\mu\)&lt;/span&gt; could also be called in many places the Lindblad operators if they yields the POVMs as defined earlier.&lt;/p&gt;
&lt;p&gt;Just a note on side: the measurement operator defined earlier is usually called POVMs (postive-valued measurements) if the eigenvalues are not negative corresponding to physical projection measurement outputs. In research, finding an optimal set of measurements to retrieve the maximum information of the quantum state of a system with minimal times of measurements has been a hot topic and leads to the field of quantum tomography, compressed sensing and related algorithms.&lt;/p&gt;
&lt;h2 id=&quot;a-second-look-on-the-quantum-trajectories-using-stochastic-calculus-language&quot;&gt;A second look on the quantum trajectories using stochastic calculus language&lt;/h2&gt;
&lt;p&gt;Above, we have described the system-environment interaction as a process of random quantum measurements. In the cases we have studied, we treat the environmental disturb as statistically identical sources over time–to some extent, the jump probability distribution has to be identical for each jump step to be able to directly apply the pure time-differential equations we have derived earlier. Let us take a closer look on its statistic nature in the language of stochastic calculus&lt;a href=&quot;#Gardiner1985Handbook&quot;&gt; [5]&lt;/a&gt;&lt;a href=&quot;#fn3&quot; class=&quot;footnoteRef&quot; id=&quot;fnref3&quot;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
For simplicity, we will stick to a given state &lt;span class=&quot;math&quot;&gt;\(\ket{\Psi(t)}\)&lt;/span&gt;. We define a random variable–a stochastic interval &lt;span class=&quot;math&quot;&gt;\(dN_\mu(t)\)&lt;/span&gt;–Poisson distributed with values
&lt;div&gt;
&lt;span class=&quot;math&quot;&gt;\[\begin{equation}\label{eq:dNmudistr}
dN_\mu(t) = \left\{
  \begin{array}
  \\1 , &amp;amp; \text{with probability } p_\mu \\
  0, &amp;amp; \text{with probability } 1-p_\mu.
  \end{array}\right.
\end{equation}\]&lt;/span&gt;
&lt;/div&gt;
&lt;p&gt;It satisfies the properties based on the rule of stochastic calculus that &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\sum_{\mu=0}^m dN_\mu(t) &amp;amp;=1,\label{eq:sumdNmu}\\
dN_\mu(t)dN_\nu (t) &amp;amp;= \delta_{\mu\nu} dN_\mu(t),\\
\text{expectation value } \langle dN_\mu (t)\rangle &amp;amp;= p_\mu = \bra{\Psi(t)} \hat{L}^\dagger_\mu \hat{L}_\mu \ket{\Psi(t)}dt.
\end{align}\]&lt;/span&gt; Then based on the physical meaning, we can rewrite an unnormalized form of the evolved state by &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\ket{\tilde{\Psi}(t+dt)} &amp;amp;= dN_0(t)\hat{M}_0\ket{\Psi(t)} + \sum_{\mu=1}^m dN_\mu (t)\hat{L}_\mu \ket{\Psi(t)}.
\end{align}\]&lt;/span&gt; We use the property of the stochastic internal &lt;span class=&quot;math&quot;&gt;\(dN_\mu(t)\)&lt;/span&gt; to replace &lt;span class=&quot;math&quot;&gt;\(dN_0(t)=1-\sum_{\mu=1}^m dN_\mu\)&lt;/span&gt;, and obtain &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\ket{\tilde{\Psi}(t+dt)} &amp;amp;= \hat{M}_0\ket{\Psi(t)} + \sum_{\mu=1}^m dN_\mu (t) (\hat{L}_\mu -\hat{M}_0) \ket{\Psi(t)}\\
&amp;amp;= (\mathbb{1} - \frac{i}{\hbar}\hat{H}_{eff}dt)\ket{\Psi(t)} + \sum_{\mu=1}^m dN_\mu (\hat{L}_\mu - \mathbb{1} + \frac{i}{\hbar}\hat{H}_{eff}dt)\ket{\Psi(t)}.
\end{align}\]&lt;/span&gt; We ignore the terms that have an order higher than &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt; according to the rules of stochastic calculus, that is to let &lt;span class=&quot;math&quot;&gt;\(dN_\mu (t)dt \sim \sqrt{dt}dt=0\)&lt;/span&gt;, and obtain the unnormalized state update equation &lt;span class=&quot;math&quot;&gt;\[\begin{align}
d\ket{\tilde{\Psi}} &amp;amp;\equiv \ket{\tilde{\Psi}(t+dt)} - \ket{\Psi(t)}\\
&amp;amp;= -\frac{i}{\hbar} \hat{H}_{eff}dt\ket{\Psi} + \sum_{\mu=1}^m dN_\mu(t)(\hat{L}_\mu-1)\ket{\Psi}.
\end{align}\]&lt;/span&gt; This is the unnormalized &lt;strong&gt;&lt;em&gt;stochastic Schrodinger equation&lt;/em&gt;&lt;/strong&gt; (SSE) for a jump update. Alternatively, in the normalized form, the SSE becomes &lt;span class=&quot;math&quot;&gt;\[\begin{align}\label{eq:SSE}
d\ket{\Psi} &amp;amp;= \left( -\frac{i}{\hbar}\hat{H}_{eff} + \frac{1}{2}\sum_{\mu=1}^m \langle \hat{L}^\dagger_\mu \hat{L}_\mu \rangle \right) dt\ket{\Psi} + \sum_{\mu=1}^m dN_\mu(t) \left( \frac{\hat{L}_\mu}{\sqrt{\langle \hat{L}^\dagger_\mu \hat{L}_\mu\rangle }}-1\right)\ket{\Psi}.
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By using the definition that the density operator is the ensemble average, &lt;span class=&quot;math&quot;&gt;\(\hat{\rho}(t+dt)=\bar{\ketbra{\Psi(t+dt)}{\Psi(t+dt)}}\)&lt;/span&gt; and only keep terms up to the order of &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt;, it is trivial to show that the Lindblad equation (Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:lindblad}\)&lt;/span&gt;) is equivalent to the SSE.&lt;/p&gt;
&lt;p&gt;There are two important properties of the SSEs, Lindblad equations and &lt;span class=&quot;math&quot;&gt;\(dN(t)\)&lt;/span&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The SSEs and Lindblad master equations in general are unlike the normal Schrodinger equation, and could be nonlinear due to the fact that &lt;span class=&quot;math&quot;&gt;\(dN_\mu(t)\)&lt;/span&gt; could depend on the state of &lt;span class=&quot;math&quot;&gt;\(\Psi(t)\)&lt;/span&gt; at each time step&lt;a href=&quot;#fn4&quot; class=&quot;footnoteRef&quot; id=&quot;fnref4&quot;&gt;&lt;sup&gt;4&lt;/sup&gt;&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;In our case, &lt;span class=&quot;math&quot;&gt;\(dN(t)\)&lt;/span&gt; obeys the Poisson distribution. It means that, in every jump of the quantum trajectory, the jump statistics will be the same as the previous step, and the process of determining which trajectory to jump can be mapped to the “toss of a coin” problem–whether the result is a “head” or “tail” for every toss is predefined as independent identical instances with a uniform random distribution. This is a typical Markov chain process that the “toss of coin” doesn’t have a memory with the previous step, or physically you might want to regard this process as being jumping in steps slower than the dissipative or equilibrium process (&lt;span class=&quot;math&quot;&gt;\(\delta t \gg 1/\gamma\)&lt;/span&gt;). Lindblad equations with a simple time-homogeneous evolution only works for Markovian processes with simple random sampling properties.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In similar cases, one can use different Lindblad operators to describe the quantum dynamics with the same form of Lindblad equations as shown in Eq.&lt;span class=&quot;math&quot;&gt;\(\eqref{eq:lindblad}\)&lt;/span&gt;. Commonly, the &lt;em&gt;relaxation&lt;/em&gt; process of a quantum system in presence of a reservoir can be described by the Lindblad superoperator &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\mathcal{L}[\hat{\rho}] &amp;amp;= -\frac{1}{2}\sum_{\mu=1} \left( \hat{L}_\mu^\dagger\hat{L}_\mu\hat{\rho}+\hat{\rho}\hat{L}^\dagger_\mu\hat{L}_\mu-2\hat{L}_\mu\hat{\rho}\hat{L}^\dagger_\mu \right)\\
&amp;amp;= \frac{1}{2}\sum_{\mu=1}\left[\hat{L}_\mu,\left[\hat{\rho}, \hat{L}_\mu \right] \right].
\end{align}\]&lt;/span&gt;&lt;/p&gt;
&lt;h2 id=&quot;all-can-be-extended-to-more-complicated-cases-with-a-small-tweak&quot;&gt;All can be extended to more complicated cases with a small tweak&lt;/h2&gt;
&lt;p&gt;We can briefly take a look on how this formalism can be extended to other cases and find the correct Lindblad operators–yes, find a proper Lindblad operator is the only modification without changing the form of equations for some cases. For example, in a homodyne detection case&lt;a href=&quot;#Wiseman1993Interpretation&quot;&gt; [6]&lt;/a&gt;, a beam splitter is used to branch out two signal paths for the final measurement; a local oscillator, which could be a coherent laser beam locked at the same frequency as the input signal on the other branch of the beam splitter as shown in the figure below. For a balanced beam splitter, the two output branches yield the combined and subtracted signals in detection, which are the &lt;span class=&quot;math&quot;&gt;\(\hat{X}\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(\hat{P}\)&lt;/span&gt; quadratures in the phase plane.&lt;/p&gt;
&lt;center&gt;
 
&lt;figure&gt;
&lt;img src=&quot;/assets/img/homodyneheterodyne_beamsplitter.svg&quot; alt=&quot;Fig 1. Beam splitter for homodyne and heterodyne detections.&quot; /&gt;&lt;figcaption&gt;Fig 1. Beam splitter for homodyne and heterodyne detections.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;br&gt; &lt;br&gt;
&lt;/center&gt;

&lt;p&gt;By treating the local oscillator as a coherent light with a large amplitude &lt;span class=&quot;math&quot;&gt;\(\alpha\)&lt;/span&gt; (related to the averge phone flux with a phase factor), the input-output relationship of the beam splitter can be given by &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\hat{a}_\pm &amp;amp;= \frac{\alpha\hat{\mathbb{1}} \pm \hat{a}}{\sqrt{2}}.
\end{align}\]&lt;/span&gt; Assuming the other part of the quantum system is the same as before and the signal has &lt;span class=&quot;math&quot;&gt;\(\mu=1,\cdots,m\)&lt;/span&gt; modes, the Lindblad operators can be written as &lt;span class=&quot;math&quot;&gt;\(\hat{J}_{\mu,\pm} = \frac{A_\mu \hat{\mathbb{1}}\pm \hat{L}_\mu}{\sqrt{2}}\)&lt;/span&gt;, where &lt;span class=&quot;math&quot;&gt;\(A_\mu\)&lt;/span&gt; is a complex constant. After some algebra, the Krause operators of the homodyne case can also be defined as a linear transformation of the previous Krause operators: &lt;span class=&quot;math&quot;&gt;\[\begin{align}
\left(\begin{array}{c} \hat{K}_0\\ \hat{K}_{\mu,+} \\ \hat{K}_{\mu,-}\end{array}\right)
&amp;amp;= \left[ \begin{array}{ccc}
1-\frac{1}{2}|A_\mu|^2dt &amp;amp; 0 &amp;amp; A_\mu \sqrt{\frac{dt}{2}}\\
A_\mu\sqrt{\frac{dt}{2}} &amp;amp; \frac{1}{\sqrt{2}} &amp;amp; \frac{1-\frac{1}{2}|A_\mu|^2dt}{\sqrt{2}}\\
A_\mu\sqrt{2} &amp;amp; -\frac{1}{\sqrt{2}} &amp;amp; \frac{1-\frac{1}{2}|A_\mu|^2dt}{\sqrt{2}}
\end{array}\right]
\left(\begin{array}{c} \hat{M}_0 \\ \hat{M}_\mu \\ 0 \end{array}\right).
\end{align}\]&lt;/span&gt; The form of Lindblad eqution and the stochastic Schrodinger equation will be the same as before&lt;a href=&quot;#Wiseman1993Interpretation&quot;&gt; [6]&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;You may ask could the sampling process have any statistical distribution types other than the uniform random distribution? Indeed, the stochastic interval &lt;span class=&quot;math&quot;&gt;\(dN_\mu(t)\)&lt;/span&gt; can have different statistical distribution modes. In either homodyne or heterodyne measurements with a local oscillator at a frequency the same as or different from the signal merged at a beam splitter&lt;a href=&quot;#Wiseman1993Interpretation&quot;&gt; [6]&lt;/a&gt;, the local oscillator injects noise to the measurement result and affects the quantum dynamics in the process of continuous measurements. Roughly speaking, we may be able to formally rewrite the stochastic interval in the following form, &lt;span class=&quot;math&quot;&gt;\[\begin{equation}
dN_\mu (t) = &amp;lt;dN_\mu(t)&amp;gt; + \sigma_\mu \delta W_\mu(t),
\end{equation}\]&lt;/span&gt; where the first part &lt;span class=&quot;math&quot;&gt;\(&amp;lt;dN_\mu(t)&amp;gt;\)&lt;/span&gt; is the mean value of stochastic interval, &lt;span class=&quot;math&quot;&gt;\(\sigma_\mu\)&lt;/span&gt; is the variance, and &lt;span class=&quot;math&quot;&gt;\(\delta W_\mu(t)\)&lt;/span&gt; is the Weiner stochastic variable. In our case, &lt;span class=&quot;math&quot;&gt;\(&amp;lt;dN_\mu(t)&amp;gt;\)&lt;/span&gt; is proportional to the intensity of the input modes, and the variance of the signals can be determined by the square root of the photon number in the interaction time interval for a coherent light. Given a relatively strong local oscillator, the noise generated obeys the central limit law and will be a Gaussian noise. Therefore, &lt;span class=&quot;math&quot;&gt;\(\delta W_\mu(t)\)&lt;/span&gt; will be a Brownian diffusion random variable in a Gaussian distribution which gives a white noise. Later, we will use &lt;span class=&quot;math&quot;&gt;\(dW(t)\)&lt;/span&gt; to characterize the stochastic process for the normalized stochastic variable with various distribution modes. We will see the stochastic viable will eventually extend the commonly seen time-homogeneous quantum evolution to a broader spectrum of scenarios in the next part of the notes.&lt;/p&gt;
&lt;p&gt;If you want to modify a quantum dynamics simulation code–either using Monte Carlo method or other algorithms–which only involves homogeneously distributed random variable to a general stochastic processes, what you need to do is to bring in a stochastic variable &lt;span class=&quot;math&quot;&gt;\(dW(t)\)&lt;/span&gt; with other statistical distributions and find a proper integration method to solve the differential equations. Using the homodyne detection case, for instance, you can define &lt;span class=&quot;math&quot;&gt;\(dt\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(dW(t)\)&lt;/span&gt; in a time list &lt;span class=&quot;math&quot;&gt;\(t\)&lt;/span&gt; as below.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;In [1]:&lt;/strong&gt;&lt;/p&gt;
&lt;pre class=&quot;sourceCode julia&quot;&gt;&lt;code class=&quot;sourceCode julia&quot;&gt;tmin = &lt;span class=&quot;fl&quot;&gt;0&lt;/span&gt;.;
tmax = &lt;span class=&quot;fl&quot;&gt;10.0&lt;/span&gt;;
step = &lt;span class=&quot;fl&quot;&gt;100&lt;/span&gt;;
t = linspace(tmin,tmax,step);
dt = t[&lt;span class=&quot;fl&quot;&gt;2&lt;/span&gt;]-t[&lt;span class=&quot;fl&quot;&gt;1&lt;/span&gt;];
dW = randn(step).*dt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The variable &lt;code&gt;dW&lt;/code&gt; is now a list of Gaussian/normally distributed numbers with mean &lt;span class=&quot;math&quot;&gt;\(0\)&lt;/span&gt; and standard deviation &lt;span class=&quot;math&quot;&gt;\(1\)&lt;/span&gt; with the same length as &lt;code&gt;t&lt;/code&gt;, and &lt;code&gt;dt&lt;/code&gt; is a constant for all time steps. Easy peasy, right?&lt;/p&gt;
&lt;h2 id=&quot;wrap-up-for-this-part&quot;&gt;Wrap up for this part&lt;/h2&gt;
&lt;p&gt;In this part of notes, we have revisited the quantum dynamics problems that has been widely taught in undergraduate level quantum mechanics classes but in a stochastic process perspective in the end. In general, the stochastic process reflects one important nature of quantum systems and will lead to a rich church of differential equations that our JuliaQuantum libraries would want to include, and I will illustrate more on their forms in the next part. The leftover practical task for solving stochastic equations is to implementing various solvers or integrating algorithms. There are two well-known stochastic integrals–&lt;a href=&quot;https://en.wikipedia.org/wiki/It%C3%B4_calculus&quot;&gt;Itô integral&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Stratonovich_integral&quot;&gt;Stratonovich integral&lt;/a&gt;. More details of the general form of stochastic differential equations and integration methods can be found in &lt;a href=&quot;#Gardiner2004Quantum&quot;&gt; [7]&lt;/a&gt; and other modern books. I will not write too much on those details, but rather give you a big picture on the connected fields.&lt;/p&gt;
&lt;p&gt;On the other hand, quantum simulation packages like the &lt;em&gt;Quantum Optics Toolbox in Matlab&lt;/em&gt;&lt;a href=&quot;#Tan1999Computational&quot;&gt; [8]&lt;/a&gt; and its Python version &lt;em&gt;QuTiP&lt;/em&gt;&lt;a href=&quot;#Johansson2013Qutip&quot;&gt; [9]&lt;/a&gt; becomes widely used while the input quantities can be classified into a few object classes is mainly due to the mathematical fact that the quantum dynamics equations and quantum systems those toolboxes can simulate all have nice mathematical structures. For example, a lot of quantum dynamics problems can be described by the Lindblad form of master equations, which in the end is completely positive maps (CP-maps) and can be fully characterized by the hierarchy of complete positive operators and superoperators as mathematical objects&lt;a href=&quot;#fn5&quot; class=&quot;footnoteRef&quot; id=&quot;fnref5&quot;&gt;&lt;sup&gt;5&lt;/sup&gt;&lt;/a&gt;. To my perspective, the existing JuliaQuantum libraries can be improved by extending its representation types to the operator and superoperator system and build more general solvers to cover the stochastic equations, and we can make better program libraries with in-depth mathematical insights and high-performance programming language. I will also touch the base of some mathematical fundation in the next part.&lt;/p&gt;
&lt;p&gt;With the basic stochastic language introduced, we are ready to explore more complicated differential equations and start appreciating the nature of quantum measurements and how all of these can be connected to the quantum circuit model. Content is mainly based on my hand-written lecture notes taught by my supervisor &lt;a href=&quot;http://cquic.unm.edu/deutsch-group/&quot;&gt;Ivan Deutsch&lt;/a&gt; and my daily journals on reading, meeting and playing. I hope the following notes can help understand those content intuitively.&lt;/p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Dum1992Monte&quot;&gt;R. Dum, A. S. Parkins, P. Zoller, and C. W. Gardiner, Physical Review A &lt;b&gt;46&lt;/b&gt;, 4382 (1992).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Dum1992Monte-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Dum1992Monte-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Dum1992Monte-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Dum1992Monte,
  title = {Monte Carlo simulation of master equations in quantum optics for vacuum, thermal, and squeezed reservoirs},
  author = {Dum, R and Parkins, AS and Zoller, P and Gardiner, CW},
  journal = {Physical Review A},
  volume = {46},
  number = {7},
  pages = {4382},
  year = {1992},
  publisher = {APS}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Molmer1993Monte&quot;&gt;K. Mølmer, Y. Castin, and J. Dalibard, JOSA B &lt;b&gt;10&lt;/b&gt;, 524 (1993).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Molmer1993Monte-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Molmer1993Monte-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Molmer1993Monte-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Molmer1993Monte,
  title = {Monte Carlo wave-function method in quantum optics},
  author = {M{\o}lmer, Klaus and Castin, Yvan and Dalibard, Jean},
  journal = {JOSA B},
  volume = {10},
  number = {3},
  pages = {524--538},
  year = {1993},
  publisher = {Optical Society of America}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Molmer1996Monte&quot;&gt;K. Mølmer and Y. Castin, Quantum and Semiclassical Optics: Journal of the European Optical Society Part B &lt;b&gt;8&lt;/b&gt;, 49 (1996).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Molmer1996Monte-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Molmer1996Monte-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Molmer1996Monte-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Molmer1996Monte,
  title = {Monte Carlo wavefunctions in quantum optics},
  author = {M{\o}lmer, Klaus and Castin, Yvan},
  journal = {Quantum and Semiclassical Optics: Journal of the European Optical Society Part B},
  volume = {8},
  number = {1},
  pages = {49},
  year = {1996},
  publisher = {IOP Publishing}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Plenio1998Quantum&quot;&gt;M. B. Plenio and P. L. Knight, Reviews of Modern Physics &lt;b&gt;70&lt;/b&gt;, 101 (1998).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Plenio1998Quantum-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Plenio1998Quantum-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Plenio1998Quantum-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Plenio1998Quantum,
  title = {The quantum-jump approach to dissipative dynamics in quantum optics},
  author = {Plenio, MB and Knight, PL},
  journal = {Reviews of Modern Physics},
  volume = {70},
  number = {1},
  pages = {101},
  year = {1998},
  publisher = {APS}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Gardiner1985Handbook&quot;&gt;C. W. Gardiner, &lt;i&gt;Handbook of Stochastic Methods&lt;/i&gt;, 2nd ed. (Springer Berlin, 1985).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Gardiner1985Handbook-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Gardiner1985Handbook-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Gardiner1985Handbook-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@book{Gardiner1985Handbook,
  title = {Handbook of stochastic methods},
  author = {Gardiner, Crispin W},
  edition = {2nd},
  year = {1985},
  publisher = {Springer Berlin}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Wiseman1993Interpretation&quot;&gt;H. M. Wiseman and G. J. Milburn, Phys. Rev. A &lt;b&gt;47&lt;/b&gt;, 1652 (1993).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Wiseman1993Interpretation-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Wiseman1993Interpretation-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;

&lt;li&gt;
&lt;a href=&quot;http://dx.doi.org/10.1103/PhysRevA.47.1652&quot; target=&quot;_blank&quot;&gt;Link&lt;/a&gt;
&lt;/li&gt;



&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Wiseman1993Interpretation-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Wiseman1993Interpretation,
  title = {Interpretation of quantum jump and diffusion processes illustrated on the Bloch sphere},
  author = {Wiseman, H. M. and Milburn, G. J.},
  journal = {Phys. Rev. A},
  volume = {47},
  issue = {3},
  pages = {1652--1666},
  numpages = {0},
  year = {1993},
  month = mar,
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevA.47.1652},
  url = {http://link.aps.org/doi/10.1103/PhysRevA.47.1652}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Gardiner2004Quantum&quot;&gt;C. Gardiner and P. Zoller, &lt;i&gt;Quantum Noise: a Handbook of Markovian and Non-Markovian Quantum Stochastic Methods with Applications to Quantum Optics&lt;/i&gt; (Springer Science &amp;amp; Business Media, 2004).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Gardiner2004Quantum-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Gardiner2004Quantum-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Gardiner2004Quantum-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@book{Gardiner2004Quantum,
  title = {Quantum noise: a handbook of Markovian and non-Markovian quantum stochastic methods with applications to quantum optics},
  author = {Gardiner, Crispin and Zoller, Peter},
  volume = {56},
  year = {2004},
  publisher = {Springer Science \&amp; Business Media}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Tan1999Computational&quot;&gt;S. M. Tan, Journal of Optics B: Quantum and Semiclassical Optics &lt;b&gt;1&lt;/b&gt;, 424 (1999).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Tan1999Computational-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Tan1999Computational-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Tan1999Computational-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Tan1999Computational,
  title = {A computational toolbox for quantum and atomic optics},
  author = {Tan, Sze M},
  journal = {Journal of Optics B: Quantum and Semiclassical Optics},
  volume = {1},
  number = {4},
  pages = {424},
  year = {1999},
  publisher = {IOP Publishing}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;!----&gt;
&lt;span id=&quot;Johansson2013Qutip&quot;&gt;J. R. Johansson, P. D. Nation, and F. Nori, Computer Physics Communications &lt;b&gt;184&lt;/b&gt;, 1234 (2013).&lt;/span&gt; &lt;br/&gt;
&lt;div id=&quot;Johansson2013Qutip-materials&quot;&gt;
&lt;ul class=&quot;nav nav-pills&quot;&gt;

&lt;li&gt;
&lt;a data-toggle=&quot;collapse&quot; href=&quot;#Johansson2013Qutip-bibtex&quot;&gt;BibTeX&lt;/a&gt;
&lt;/li&gt;




&lt;!----&gt;

&lt;/ul&gt;

&lt;pre id=&quot;Johansson2013Qutip-bibtex&quot; class=&quot;pre pre-scrollable collapse&quot;&gt;@article{Johansson2013Qutip,
  title = {QuTiP 2: A Python framework for the dynamics of open quantum systems},
  author = {Johansson, JR and Nation, PD and Nori, Franco},
  journal = {Computer Physics Communications},
  volume = {184},
  number = {4},
  pages = {1234--1240},
  year = {2013},
  publisher = {Elsevier}
}
&lt;/pre&gt;
&lt;/div&gt;
&lt;/li&gt;&lt;/ol&gt;

&lt;div class=&quot;references&quot;&gt;
&lt;h2&gt;Footnotes&lt;/h2&gt;
&lt;/div&gt;
&lt;section class=&quot;footnotes&quot;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&quot;fn1&quot;&gt;&lt;p&gt;If you only know quantum operators, superoperators are just another layer of operations on operators. Superoperator notations have been widely used to describe the evolution of open quantum systems where the concept of propagator can be fully characterized.&lt;a href=&quot;#fnref1&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn2&quot;&gt;&lt;p&gt;For mixed states, the state of the system cannot be written as a signle pure state, &lt;span class=&quot;math&quot;&gt;\(\ket{\Psi}\)&lt;/span&gt; any more, in general. Instead, a density operator &lt;span class=&quot;math&quot;&gt;\(\hat{\rho}=\sum_i p_i\ketbra{\Psi_i}{\Psi_i}\)&lt;/span&gt; as an ensemble decomposition of pure states, &lt;span class=&quot;math&quot;&gt;\(\ket{\Psi_i}\)&lt;/span&gt;, is used to characterize the system, where &lt;span class=&quot;math&quot;&gt;\(p_i\)&lt;/span&gt; is the probability of being in the &lt;span class=&quot;math&quot;&gt;\(\ket{\Psi_i}\)&lt;/span&gt; state.&lt;a href=&quot;#fnref2&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn3&quot;&gt;&lt;p&gt;Also see the &lt;a href=&quot;https://en.wikipedia.org/wiki/Quantum_stochastic_calculus&quot;&gt;Quantum stochastic calculus&lt;/a&gt; page for a quick review created by &lt;a href=&quot;http://www.unm.edu/~jagross/&quot;&gt;Jonathan Gross&lt;/a&gt; for a &lt;a href=&quot;http://iciq.github.io/entangle/WikipediaProject.html&quot;&gt;Quantum Optics wikipedia project&lt;/a&gt;.&lt;a href=&quot;#fnref3&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn4&quot;&gt;&lt;p&gt;We will use this fact in my future notes on deriving quantum dynamic equations symbolically for general scenarios.&lt;a href=&quot;#fnref4&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&quot;fn5&quot;&gt;&lt;p&gt;A brief summary on the relation between Lindblad equations and CP-map can be found in &lt;a href=&quot;http://blog.jessriedel.com/2014/07/26/lindblad-equation-is-differential-form-of-cp-map/&quot;&gt;Jess Riedel’s blog&lt;/a&gt;.&lt;a href=&quot;#fnref5&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</content>
 </entry>
 
 <entry>
   <title>Dispersion relations for linear systems of PDEs</title>
   <link href="/2014/05/28/dispersion_relations.html"/>
   <updated>2014-05-28T00:00:00+00:00</updated>
   <id>h/2014/05/28/dispersion_relations</id>
   <content type="html">&lt;p&gt;Note: this post was originally written by &lt;a href=&quot;http://www.davidketcheson.info/2014/05/28/dispersion_relations.html&quot;&gt;David Ketcheson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Fourier analysis is an essential tool for understanding the behavior of solutions to linear equations. Often, this analysis is introduced to students in the context of scalar equations with real coefficients. If nothing more is said, students may mistakenly apply assumptions based on the scalar case to systems, leading to erroneous conclusions. I’m surprised at how often I’ve seen this, and I’ve even made the mistake myself.&lt;/p&gt;
&lt;h2 id=&quot;scalar-equations&quot;&gt;Scalar equations&lt;/h2&gt;
&lt;p&gt;Students in any undergraduate PDE course learn that solutions of the heat equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[
\label{heat}
u_t(x,t) = u_{xx}(x,t)
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;diffuse in time whereas solutions of the wave equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[
\label{wave}
u_{tt} = u_{xx}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;oscillate in time without growing or decaying. They may even be introduced to a general approach for the Cauchy problem: given an evolution equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \label{evol}
u_t = \sum_{j=0}^n a_j \frac{\partial^j u}{\partial x^j},
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;one inserts the Fourier mode solution&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[ \label{fourier}
u(x,t) = e^{i(kx - \omega(k) t)}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;to obtain&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[-i\omega(k) = \sum_{j=0}^n a_j (ik)^j\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;or simply&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[\omega(k) = \sum_{j=0}^n a_j i^{j+1} k^j.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The function &lt;span class=&quot;math&quot;&gt;\(\omega(k)\)&lt;/span&gt; is often referred to as the &lt;em&gt;dispersion relation&lt;/em&gt; for the PDE. Any solution can be expressed as a sum of Fourier modes, and each mode propagates in a manner dictated by the dispersion relation. It’s easy to see that&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If &lt;span class=&quot;math&quot;&gt;\(\omega(k)\)&lt;/span&gt; is &lt;strong&gt;real&lt;/strong&gt;, then energy is conserved and each mode simply translates. This occurs if only odd-numbered spatial derivatives appear in the evolution equation \eqref{evol}.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&quot;math&quot;&gt;\(\omega(k)\)&lt;/span&gt; has &lt;strong&gt;negative imaginary part&lt;/strong&gt;, energy decays in time. The heat equation \eqref{heat} behaves this way.&lt;/li&gt;
&lt;li&gt;If &lt;span class=&quot;math&quot;&gt;\(\omega(k)\)&lt;/span&gt; has &lt;strong&gt;positive imaginary part&lt;/strong&gt;, then the energy will grow exponentially in time. This doesn’t usually occur in physical systems. An example of this behavior is obtained by changing the sign of the right side in the heat equation to get &lt;span class=&quot;math&quot;&gt;\(u_t = - u_{xx}\)&lt;/span&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What about the wave equation, which has two time derivatives? Using the same Fourier mode ansatz \eqref{fourier}, one obtains &lt;span class=&quot;math&quot;&gt;\[
\begin{align}
\omega^2 &amp;amp; = k^2
\end{align}
\]&lt;/span&gt; or &lt;span class=&quot;math&quot;&gt;\(\omega = \pm k\)&lt;/span&gt;. Since &lt;span class=&quot;math&quot;&gt;\(\omega\)&lt;/span&gt; is real, energy is conserved.&lt;/p&gt;
&lt;p&gt;In the discussion above, we have assumed that &lt;span class=&quot;math&quot;&gt;\(u\)&lt;/span&gt; is a scalar and that the coefficients &lt;span class=&quot;math&quot;&gt;\(a_j\)&lt;/span&gt; are real. Many undergraduate courses stop at this point, and students are left with the intuition that &lt;strong&gt;even-numbered derivative terms are diffusive&lt;/strong&gt; while &lt;strong&gt;odd-numbered derivative terms are dispersive&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;In practice, we often deal with systems of PDEs or PDEs with complex coefficients, and this intuition is then no longer correct. There is nothing deep or mysterious about this topic, but it’s easy to jump to incorrect conclusions if one is not careful. To take a common example, consider the time-dependent Schroedinger equation: &lt;span class=&quot;math&quot;&gt;\[i \psi_t = \psi_{xx} + V\psi.\]&lt;/span&gt; At first glance, we have on the right side a diffusion term (&lt;span class=&quot;math&quot;&gt;\(\psi_{xx}\)&lt;/span&gt;) and a reaction term (&lt;span class=&quot;math&quot;&gt;\(V\psi\)&lt;/span&gt;). But what about that pesky factor of &lt;span class=&quot;math&quot;&gt;\(i\)&lt;/span&gt; (the imaginary unit) on the left hand side? It’s easy to find the answer using the usual ansatz, but let’s take a little detour first.&lt;/p&gt;
&lt;h2 id=&quot;systems-of-equations&quot;&gt;Systems of equations&lt;/h2&gt;
&lt;p&gt;Consider the linear system &lt;span class=&quot;math&quot;&gt;\[
\begin{align*}
u_t = A  \frac{\partial^j u}{\partial x^j},
\end{align*}
\]&lt;/span&gt; where &lt;span class=&quot;math&quot;&gt;\(u\in \mathbb{R}^m\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; is a square real matrix. Let &lt;span class=&quot;math&quot;&gt;\(\lambda_m\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(s_m\)&lt;/span&gt; denote the eigenvalues and eigenvectors (respectively) of &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt;. Inserting the Fourier mode solution &lt;span class=&quot;math&quot;&gt;\[u(x,t) = s_m e^{i(kx - \omega(k) t)},\]&lt;/span&gt; we obtain &lt;span class=&quot;math&quot;&gt;\[\omega(k) = i^{j+1} k^j \lambda_m s_m,\]&lt;/span&gt; and any solution can be written as a superposition of these. We see now that the behavior of the energy with respect to time depends on both the number &lt;span class=&quot;math&quot;&gt;\(j\)&lt;/span&gt; of spatial derivatives and the nature of the eigenvalues of &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt;. For instance, if &lt;span class=&quot;math&quot;&gt;\(j=1\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; has imaginary eigenvalues, energy is conserved. We can obtain just such an example by rewriting the wave equation \eqref{wave} as a first-order system: &lt;span class=&quot;math&quot;&gt;\[
\begin{align}
u_t &amp;amp; = v_x \label{w1} \\
v_t &amp;amp; = u_x. \label{w2}
\end{align}
\]&lt;/span&gt; (If you’re not familiar with this, just differentiate \eqref{w1} w.r.t. &lt;span class=&quot;math&quot;&gt;\(t\)&lt;/span&gt; and \eqref{w2} w.r.t. &lt;span class=&quot;math&quot;&gt;\(x\)&lt;/span&gt;, then equate partial derivatives to get back the second-order wave equation \eqref{wave}). We have a linear system with &lt;span class=&quot;math&quot;&gt;\(j=1\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\[ A = \begin{pmatrix}
0 &amp;amp; 1 \\ 1 &amp;amp; 0
\end{pmatrix}.\]&lt;/span&gt; This matrix has eigenvalues &lt;span class=&quot;math&quot;&gt;\(\lambda=\pm 1\)&lt;/span&gt;, so &lt;span class=&quot;math&quot;&gt;\(\omega(k)\)&lt;/span&gt; has zero imaginary part.&lt;/p&gt;
&lt;p&gt;In this example, our intuition from the scalar case works: our first-order system, with only odd-numbered derivatives, leads to wave-like behavior. But notice that if &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; had imaginary eigenvalues, our intuition would be wrong; for instance, the system &lt;span class=&quot;math&quot;&gt;\[
\begin{align*}
u_t &amp;amp; = -v_x \\
v_t &amp;amp; = u_x,
\end{align*}
\]&lt;/span&gt; corresponding to the second-order equation &lt;span class=&quot;math&quot;&gt;\(u_{tt} = - u_{xx},\)&lt;/span&gt; admits exponentially growing solutions.&lt;/p&gt;
&lt;h2 id=&quot;scalar-problems-with-complex-coefficients&quot;&gt;Scalar problems with complex coefficients&lt;/h2&gt;
&lt;p&gt;Now that we understand the dispersion relation for systems, it’s easy to understand the dispersion relation for the Schrodinger equation. Multiply by &lt;span class=&quot;math&quot;&gt;\(-i\)&lt;/span&gt; to get &lt;span class=&quot;math&quot;&gt;\[\psi_t = -i\psi_{xx} + -iV\psi.\]&lt;/span&gt; Now we can think of this in the same way as a system, where the coefficient matrices have purely imaginary eigenvalues. Then it’s clear that the (even-derivative) terms on the right hand side are both related to wave behavior (i.e., energy is conserved).&lt;/p&gt;
&lt;h2 id=&quot;systems-with-derivatives-of-different-orders&quot;&gt;Systems with derivatives of different orders&lt;/h2&gt;
&lt;p&gt;In the most general case, we have systems of linear PDEs with multiple spatial derivatives of different order: &lt;span class=&quot;math&quot;&gt;\[ \label{gensys}
u_t = \sum_{j=0}^n A_j  \frac{\partial^j u}{\partial x^j}.
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Here’s a real example from my research. It comes from homogenization of the wave equation in a spatially varying medium (see Equation (5.17) of &lt;a href=&quot;http://faculty.washington.edu/rjl/pubs/solitary/40815.pdf&quot;&gt;this paper&lt;/a&gt; for more details). It’s the wave equation plus some second-derivative terms: &lt;span class=&quot;math&quot;&gt;\[
u_t = v_x + v_{xx} \\
v_t = u_x - u_{xx}.
\]&lt;/span&gt; You might (if you hadn’t read the example above) assume that this system is dissipative due to the second derivatives. This system is of the form \eqref{gensys} with &lt;span class=&quot;math&quot;&gt;\[
\begin{align}
A_1 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \\ 1 &amp;amp; 0
\end{pmatrix}
&amp;amp;
A_2 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \\ -1 &amp;amp; 0
\end{pmatrix}.
\end{align}
\]&lt;/span&gt; Of course, &lt;span class=&quot;math&quot;&gt;\(A_1\)&lt;/span&gt; has real eigenvalues and leads to wave-like behavior. But &lt;span class=&quot;math&quot;&gt;\(A_2\)&lt;/span&gt; has pure imaginary eigenvalues, so it also leads to wave-like behavior! The second derivative terms are &lt;em&gt;dispersive&lt;/em&gt;. In fact, it’s easy to show that the energy &lt;span class=&quot;math&quot;&gt;\(E=u^2+v^2\)&lt;/span&gt; is a conserved quantity for this system (try it!).&lt;/p&gt;
&lt;p&gt;Strictly speaking, Fourier analysis like what we’ve described can’t usually be applied to \eqref{gensys} because the matrices &lt;span class=&quot;math&quot;&gt;\(A_j\)&lt;/span&gt; will not generally be simultaneously diagonalizable (though this analysis can still give us intuition for what each set of terms may do). Worse yet, the individual matrices may not be diagonalizable. Let’s illustrate with a simple case.&lt;/p&gt;
&lt;p&gt;Returning to the wave equation, let’s consider a different way of writing it as a system: &lt;span class=&quot;math&quot;&gt;\[
\begin{align*}
u_t &amp;amp; = v \\
v_t &amp;amp; = u_{xx}.
\end{align*}
\]&lt;/span&gt; It’s easy to check that this system is equivalent to the wave equation – but notice that it’s composed of parts with only even derivatives! (&lt;em&gt;reaction&lt;/em&gt; and &lt;em&gt;diffusion&lt;/em&gt; equations in the terminology of scalar PDEs). This system is of the form \eqref{gensys} with &lt;span class=&quot;math&quot;&gt;\[
\begin{align}
A_0 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 1 \\ 0 &amp;amp; 0
\end{pmatrix}
&amp;amp;
A_2 &amp;amp; = \begin{pmatrix}
0 &amp;amp; 0 \\ 1 &amp;amp; 0
\end{pmatrix}.
\end{align}
\]&lt;/span&gt; Notice that both eigenvalues of both matrices are equal to zero.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>The Schrodinger equation is not a reaction-diffusion equation</title>
   <link href="/2014/02/22/schrodinger-is-not-diffusion.html"/>
   <updated>2014-02-22T00:00:00+00:00</updated>
   <id>h/2014/02/22/schrodinger-is-not-diffusion</id>
   <content type="html">&lt;p&gt;Note: this post was originally written by &lt;a href=&quot;http://www.davidketcheson.info/2014/02/22/schrodinger-is-not-diffusion.html&quot;&gt;David Ketcheson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Recently, a stackexchange answer claimed that &lt;a href=&quot;http://scicomp.stackexchange.com/a/10878/123&quot;&gt;the Schrodinger equation is effectively a reaction-diffusion equation&lt;/a&gt;. I’ll set aside semantic arguments about the meaning of “effectively”, and give a more obvious example to explain why I think this statement is misleading.&lt;/p&gt;
&lt;p&gt;Consider the wave equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[u_{tt} = u_{xx}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Introducing a new variable &lt;span class=&quot;math&quot;&gt;\(v=u_t\)&lt;/span&gt; we can rewrite the wave equation as&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[
\begin{align*}
v_t &amp;amp; = u_{xx} \\
u_t &amp;amp; = v.
\end{align*}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Observe that the first of these equation is the diffusion equation, while the second is a reaction equation. Thus we have reaction-diffusion!&lt;/p&gt;
&lt;p&gt;Right?&lt;/p&gt;
&lt;p&gt;Wrong. We’ve disguised the true nature of this equation by applying our intuition (which is based on scalar PDEs) to a system of PDEs. In the same way, the “reaction-diffusion” label for Schrodinger is obtained by applying intuition based on PDEs with real coefficients to a PDE with complex coefficients.&lt;/p&gt;
&lt;p&gt;Of course, in both cases you can use numerical methods that are appropriate for reaction-diffusion problems in order to solve a wave equation.&lt;br /&gt;&lt;a href=&quot;http://nbviewer.ipython.org/github/ketch/exposition/blob/master/Wave%20equation%20as%20reaction-diffusion.ipynb&quot;&gt;Here is a quick ipython notebook implementation of the obvious method for the system above&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>The parallel EPPEER code</title>
   <link href="/2012/10/17/eppeer.html"/>
   <updated>2012-10-17T00:00:00+00:00</updated>
   <id>h/2012/10/17/eppeer</id>
   <content type="html">&lt;p&gt;Note: this post was originally written by &lt;a href=&quot;http://www.davidketcheson.info/2012/10/17/eppeer.html&quot;&gt;David Ketcheson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I tried out the EPPEER code, which uses two-step Runge-Kutta methods and OpenMP, because I’m thinking of writing a shared-memory parallel ODE solver code myself.&lt;/p&gt;
&lt;p&gt;I downloaded the code from&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.mathematik.uni-marburg.de/~schmitt/peer/eppeer.zip&quot; title=&quot;Go to wiki page&quot;&gt;http://www.mathematik.uni-marburg.de/~schmitt/peer/eppeer.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;unzipped, and ran&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;gfortran -c mbod4h.f90
gfortran -c ivprkp.f90
gfortran -c -fopenmp ivpepp.f90
gfortran -fopenmp ivprkp.o ivpepp.o mbod4h.o ivp_pmain.f90
./a.out&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I had to fix one line that was trying to open a logfile and failed. I also set&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export OMP_NUM_THREADS=4&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This runs the code with increasingly tight tolerances on a 400-body problem. The output was (I killed it before it finished the really tight tolerance run(s)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; tol, err, otime, cpu  0.10E-01 0.10702      2.9556      10.534    
 steps,rej,nfcn:  337   88     1399
 tol, err, otime, cpu  0.10E-02 0.93692E-01  4.9853      18.585    
 steps,rej,nfcn:  605  159     2465
 tol, err, otime, cpu  0.10E-03 0.66604E-01  7.9798      30.365    
 steps,rej,nfcn:  994  244     4015
 tol, err, otime, cpu  0.10E-04 0.47637E-01  12.026      46.477    
 steps,rej,nfcn: 1534  324     6175
 tol, err, otime, cpu  0.10E-05 0.24241E-01  18.239      70.756    
 steps,rej,nfcn: 2338  415     9391&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If I understand correctly, the last column is total CPU time; the next to last is wall time. For comparison, I ran it without parallelism:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;export OMP_NUM_THREADS=1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then I got the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; tol, err, otime, cpu  0.10E-01 0.10702      10.382      10.382    
 steps,rej,nfcn:  337   88     1399
 tol, err, otime, cpu  0.10E-02 0.93692E-01  18.297      18.297    
 steps,rej,nfcn:  605  159     2465
 tol, err, otime, cpu  0.10E-03 0.66604E-01  29.814      29.815    
 steps,rej,nfcn:  994  244     4015
 tol, err, otime, cpu  0.10E-04 0.47637E-01  45.854      45.855    
 steps,rej,nfcn: 1534  324     6175
 tol, err, otime, cpu  0.10E-05 0.24241E-01  69.725      69.726    
 steps,rej,nfcn: 2338  415     9391
 tol, err, otime, cpu  0.10E-06 0.53727E-02  105.47      105.48    
 steps,rej,nfcn: 3539  484    14195&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The numbers of function evaluations were identical, confirming that the computations being performed were the same. The speedup (about 3x) is very nice. We should be able to achieve something similar with extrapolation.&lt;/p&gt;
&lt;p&gt;These results are actually plotted in &lt;a href=&quot;http://www.mathematik.uni-marburg.de/~schmitt/peer/man_epp.pdf&quot;&gt;the user guide&lt;/a&gt;, at the end of Section 4.&lt;/p&gt;
&lt;p&gt;This was originally posted on &lt;a href=&quot;https://mathwiki.kaust.edu.sa/david/eppeer&quot;&gt;mathwiki&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Blogging with notebooks and Jekyll</title>
   <link href="/2012/10/11/blogging-with-notebooks-and-jekyll.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/blogging-with-notebooks-and-jekyll</id>
   <content type="html">&lt;p&gt;First off, if you know &lt;a href=&quot;https://github.com/yihui/knitr-jekyll&quot;&gt;KnitR&lt;/a&gt;, you can write code in R markdown directly and then compile in R/RStudio to generate the blog post easily. My blog repo has the knitr plugin installed and you can use &lt;code&gt;Python&lt;/code&gt;, &lt;code&gt;Mathlab&lt;/code&gt; and even &lt;code&gt;Julia&lt;/code&gt; engines to compile code lines. More information can be found in &lt;a href=&quot;http://yihui.name/knitr/&quot;&gt;Yihui Xie’s site&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Below, I will mention some methods that I use to blog with Jupyter notebooks with &lt;code&gt;Julia&lt;/code&gt;, &lt;code&gt;Python&lt;/code&gt; and &lt;code&gt;R&lt;/code&gt; based on the built in functionality of this &lt;a href=&quot;https://github.com/i2000s/i2000s.github.io&quot;&gt;Jekyll repo&lt;/a&gt; firstly introduced by Yihui Xie and &lt;a href=&quot;http://www.carlboettiger.info/&quot;&gt;Carl Boettiger&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In short, you can write ideas in Jupyter (iPython) notebooks and then output in Jykell markdown blog posts in various ways while keep tracking the edit history using version control.&lt;/p&gt;
&lt;p&gt;The most easiest way is to post your Jupyter notebook on GitGist, which has a preview function to Jupyter notebooks and can record your version history easily. I have a &lt;code&gt;notebooks.html&lt;/code&gt; template in my &lt;a href=&quot;https://github.com/i2000s/i2000s.github.io&quot;&gt;repo&lt;/a&gt; which automatically retrieves links to all your notebook gist repos and shows them on one page. This has been developed and used by &lt;a href=&quot;http://www.davidketcheson.info/&quot;&gt;David Ketcheson&lt;/a&gt; and others.&lt;/p&gt;
&lt;p&gt;The second method is to put your Jupyter notebooks in the blog repo and then export the markdown version for posting online. A lot of scripts have been written to make this converting process easy to commit. Here are two examples.&lt;/p&gt;
&lt;p&gt;One is introduced in &lt;a href=&quot;http://cscorley.github.io/2014/02/21/blogging-with-ipython-and-jekyll/&quot;&gt;Christop Corley’s blog&lt;/a&gt; to export markdown to your draft folder. I have the customized code in my &lt;a href=&quot;https://github.com/i2000s/i2000s.github.io&quot;&gt;repo&lt;/a&gt; in the &lt;code&gt;notebooks&lt;/code&gt; directory.&lt;/p&gt;
&lt;p&gt;The other one was introduced by &lt;a href=&quot;http://www.davidketcheson.info/2012/10/11/blogging_ipython_notebooks_with_jekyll.html&quot;&gt;David Ketcheson&lt;/a&gt;. I will just repost his original blog below for a quick preview. Please comment on his blog if you have question regarding this code. In the mean time, I have customized the &lt;code&gt;nbconv.sh&lt;/code&gt; code in my &lt;a href=&quot;https://github.com/i2000s/i2000s.github.io&quot;&gt;repo&lt;/a&gt; to work better with importing images within the notebook folder.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;David&lt;/em&gt;&lt;/strong&gt;: I’ve been playing around with &lt;a href=&quot;http://ipython.org/ipython-doc/dev/interactive/htmlnotebook.html&quot;&gt;iPython notebooks&lt;/a&gt; for a while and planning to use them instead of &lt;a href=&quot;http://www.sagemath.org/&quot;&gt;SAGE&lt;/a&gt; worksheets for my numerical analysis course next spring. As a warmup, I wrote an iPython notebook explaining a bit about internal stability of Runge-Kutta methods and showing some new research results using &lt;a href=&quot;http://numerics.kaust.edu.sa/nodepy/&quot;&gt;NodePy&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I also wanted to post the notebook on my blog here; the ability to more easily include math and code in blog posts was one of my main motivations for moving away from Blogger to my own site. I first tried following &lt;a href=&quot;http://blog.fperez.org/2012/09/blogging-with-ipython-notebook.html&quot;&gt;the instructions given by Fernando Perez&lt;/a&gt;. That was quite painless and worked flawlessly, using &lt;code&gt;nbconvert.py&lt;/code&gt; to convert the .ipynb file directly to HTML, with graphics embedded. The only issue was that I didn’t love the look of the output quite as much as I love how Carl Boettiger’s Markdown + Jekyll posts with code and math look (see an example &lt;a href=&quot;http://www.carlboettiger.info/2012/09/14/analytic-solution-to-multiple-uncertainty.html&quot;&gt;here&lt;/a&gt;). Besides, Markdown is so much nicer than HTML, and &lt;code&gt;nbconvert.py&lt;/code&gt; has a Markdown output option.&lt;/p&gt;
&lt;p&gt;So I tried the markdown option:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nbconvert.py my_nb.ipynb -f markdown&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;I copied the result to my &lt;code&gt;_posts/&lt;/code&gt; directory, added the &lt;a href=&quot;https://github.com/mojombo/jekyll/wiki/YAML-Front-Matter&quot;&gt;YAML front-matter&lt;/a&gt; that Jekyll expects, and took a look. Everything was great except that all my plots were gone, of course. After considering a few options, I decided for now to put plots for such posts in a subfolder &lt;code&gt;jekyll_images/&lt;/code&gt; of my public Dropbox folder. Then it was a simple matter of search/replace all the paths to the images. At that point, it looked great; you can see the &lt;a href=&quot;https://github.com/ketch/nodepy/blob/master/examples/Internal_stability.ipynb&quot;&gt;source&lt;/a&gt; and the &lt;a href=&quot;http://davidketcheson.info/2012/10/11/Internal_stability.html&quot;&gt;result&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The only issue was that I didn’t want to manually do all that work every time. I considered creating a new Converter class in &lt;code&gt;nbconvert&lt;/code&gt; to handle it, but finally decided that it would be more convenient to just write a shell script that calls &lt;code&gt;nbconvert&lt;/code&gt; and then operates on the result.&lt;br /&gt;Here it is:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;#!/bin/bash

fname=$1

nbconvert.py ${fname}.ipynb -f markdown
sed  -i &amp;#39;&amp;#39; &amp;quot;s#${fname}_files#https:\/\/dl.dropbox.com\/u\/656693\/jekyll_images\/${fname}_files#g&amp;quot;  ${fname}.md

dt=$(date &amp;quot;+%Y-%m-%d&amp;quot;)

echo &amp;quot;0a
---
layout:    post
time:      ${dt}
title:     TITLE-ME
subtitle:  SUBTITLE-ME
tags:      TAG-ME
---
.
w&amp;quot; | ed ${fname}.md

mv ${fname}.md ~/labnotebook/_posts/${dt}-${fname}.md&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It’s also on Github &lt;a href=&quot;https://github.com/ketch/labnotebook/blob/master/nbconv.sh&quot;&gt;here&lt;/a&gt;. This was a nice educational exercise in constructing shell scripts, in which I learned or re-learned:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;how to use command-line arguments&lt;/li&gt;
&lt;li&gt;how to use sed and ed&lt;/li&gt;
&lt;li&gt;how to use data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can expect a lot more iPython-notebook based posts in the future.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>Internal stability of Runge-Kutta methods</title>
   <link href="/2012/10/11/Internal_stability.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/Internal_stability</id>
   <content type="html">&lt;p&gt;Note: this post was generated from an iPython notebook. You can &lt;a href=&quot;https://github.com/ketch/nodepy/blob/master/examples/Internal_stability.ipynb&quot;&gt;download the notebook from github&lt;/a&gt; and execute all the code yourself. This post was originally written by &lt;a href=&quot;http://www.davidketcheson.info/2012/10/11/Internal_stability.html&quot;&gt;David Ketcheson&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Internal stability deals with the growth of errors (such as roundoff) introduced at the Runge-Kutta stages during a single Runge-Kutta step. It is usually important only for methods with a large number of stages, since that is when the internal amplification factors can be large. An excellent explanation of internal stability is given in &lt;a href=&quot;http://oai.cwi.nl/oai/asset/1652/1652A.pdf&quot;&gt;this paper&lt;/a&gt;. Here we demonstrate some tools for studying internal stability in NodePy.&lt;/p&gt;
&lt;p&gt;First, let’s load a couple of RK methods:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;nodepy&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;reload&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadRKM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;RK44&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loadRKM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;#39;SSP104&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;Classical RK4
The original four-stage, fourth-order method of Kutta
 0   |
 1/2 |  1/2
 1/2 |  0    1/2
 1   |  0    0    1
_____|____________________
     |  1/6  1/3  1/3  1/6
SSPRK(10,4)
The optimal ten-stage, fourth order SSP Runge-Kutta method
 0   |
 1/6 |  1/6
 1/3 |  1/6   1/6
 1/2 |  1/6   1/6   1/6
 2/3 |  1/6   1/6   1/6   1/6
 1/3 |  1/15  1/15  1/15  1/15  1/15
 1/2 |  1/15  1/15  1/15  1/15  1/15  1/6
 2/3 |  1/15  1/15  1/15  1/15  1/15  1/6   1/6
 5/6 |  1/15  1/15  1/15  1/15  1/15  1/6   1/6   1/6
 1   |  1/15  1/15  1/15  1/15  1/15  1/6   1/6   1/6   1/6
_____|____________________________________________________________
     |  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10  1/10&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&quot;absolute-stability-regions&quot;&gt;Absolute stability regions&lt;/h2&gt;
&lt;p&gt;First we can use NodePy to plot the region of absolute stability for each method. The absolute stability region is the set&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(\\{ z \in C : |\phi (z)|\le 1 \\}\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;where &lt;span class=&quot;math&quot;&gt;\(\phi(z)\)&lt;/span&gt; is the &lt;em&gt;stability function&lt;/em&gt; of the method:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(1 + z b^T (I-zA)^{-1}\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;If we solve &lt;span class=&quot;math&quot;&gt;\(u&amp;#39;(t) = \lambda u\)&lt;/span&gt; with a given method, then &lt;span class=&quot;math&quot;&gt;\(z=\lambda \Delta t\)&lt;/span&gt; must lie inside this region or the computation will be unstable.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stability_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_stability_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;         4          3       2
0.04167 x + 0.1667 x + 0.5 x + 1 x + 1&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_00.png&quot; /&gt;
&lt;/figure&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stability_function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot_stability_region&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;           10             9            8             7           6
3.969e-09 x  + 2.381e-07 x + 6.43e-06 x + 0.0001029 x + 0.00108 x
            5           4          3       2
 + 0.00787 x + 0.04167 x + 0.1667 x + 0.5 x + 1 x + 1&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_01.png&quot; /&gt;
&lt;/figure&gt;
&lt;h1 id=&quot;internal-stability&quot;&gt;Internal stability&lt;/h1&gt;
&lt;p&gt;The stability function tells us by how much errors from one step are amplified in the next one. This is important since we introduce truncation errors at every step. However, we also introduce roundoff errors at the each stage within a step. Internal stability tells us about the growth of those. Internal stability is typically less important than (step-by-step) absolute stability for two reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Roundoff errors are typically much smaller than truncation errors, so moderate amplification of them typically is not significant&lt;/li&gt;
&lt;li&gt;Although the propagation of stage errors within a step is governed by internal stability functions, in later steps these errors are propagated according to the (principal) stability function&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Nevertheless, in methods with many stages, internal stability can play a key role.&lt;/p&gt;
&lt;p&gt;Questions: &lt;em&gt;In the solution of PDEs, large spatial truncation errors enter at each stage. Does this mean internal stability becomes more significant? How does this relate to stiff accuracy analysis and order reduction?&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;internal-stability-functions&quot;&gt;Internal stability functions&lt;/h2&gt;
&lt;p&gt;We can write the equations of a Runge-Kutta method compactly as&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(y = u^n e + h A F(y)\)&lt;/span&gt;
&lt;/center&gt;

&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(u^{n+1} = u^n + h b^T F(y),\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;where &lt;span class=&quot;math&quot;&gt;\(y\)&lt;/span&gt; is the vector of stage values, &lt;span class=&quot;math&quot;&gt;\(u^n\)&lt;/span&gt; is the previous step solution, &lt;span class=&quot;math&quot;&gt;\(e\)&lt;/span&gt; is a vector with all entries equal to 1, &lt;span class=&quot;math&quot;&gt;\(h\)&lt;/span&gt; is the step size, &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(b\)&lt;/span&gt; are the coefficients in the Butcher tableau, and &lt;span class=&quot;math&quot;&gt;\(F(y)\)&lt;/span&gt; is the vector of stage derivatives. In floating point arithmetic, roundoff errors will be made at each stage. Representing these errors by a vector &lt;span class=&quot;math&quot;&gt;\(r\)&lt;/span&gt;, we have&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(y = u^n e + h A F(y) + r.\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;Considering the test problem &lt;span class=&quot;math&quot;&gt;\(F(y)=\lambda y\)&lt;/span&gt; and solving for &lt;span class=&quot;math&quot;&gt;\(y\)&lt;/span&gt; gives&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(y = u^n (I-zA)^{-1}e + (I-zA)^{-1}r,\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;where &lt;span class=&quot;math&quot;&gt;\(z=h\lambda\)&lt;/span&gt;. Substituting this result in the equation for &lt;span class=&quot;math&quot;&gt;\(u^{n+1}\)&lt;/span&gt; gives&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(u^{n+1} = u^n (1 + zb^T(I-zA)^{-1}e) + zb^T(I-zA)^{-1}r = \psi(z) u^n + \theta(z)^T r.\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;Here &lt;span class=&quot;math&quot;&gt;\(\psi(z)\)&lt;/span&gt; is the &lt;em&gt;stability function&lt;/em&gt; of the method, that we already encountered above. Meanwhile, the vector &lt;span class=&quot;math&quot;&gt;\(\theta(z)\)&lt;/span&gt; contains the &lt;em&gt;internal stability functions&lt;/em&gt; that govern the amplification of roundoff errors &lt;span class=&quot;math&quot;&gt;\(r\)&lt;/span&gt; within a step:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(\theta(z) = z b^T (I-zA)^{-1}.\)&lt;/span&gt;
&lt;/center&gt;

&lt;p&gt;Let’s compute &lt;span class=&quot;math&quot;&gt;\(\theta\)&lt;/span&gt; for the classical RK4 method:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_polynomials&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    [poly1d([1/24, 1/12, 1/6, 1/6, 0], dtype=object),
     poly1d([1/12, 1/6, 1/3, 0], dtype=object),
     poly1d([1/6, 1/3, 0], dtype=object),
     poly1d([1/6, 0], dtype=object)]
&lt;/pre&gt;


&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_j&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;theta_j&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;         4           3          2
0.04167 x + 0.08333 x + 0.1667 x + 0.1667 x
         3          2
0.08333 x + 0.1667 x + 0.3333 x
        2
0.1667 x + 0.3333 x

0.1667 x&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Thus the roundoff errors in the first stage are amplified by a factor &lt;span class=&quot;math&quot;&gt;\(z^4/24 + z^3/12 + z^2/6 + z/6\)&lt;/span&gt;, while the errors in the last stage are amplified by a factor &lt;span class=&quot;math&quot;&gt;\(z/6\)&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&quot;internal-instability&quot;&gt;Internal instability&lt;/h2&gt;
&lt;p&gt;Usually internal stability is unimportant since it relates to amplification of roundoff errors, which are very small. Let’s think about when things can go wrong in terms of internal instability. If &lt;span class=&quot;math&quot;&gt;\(|\theta(z)|\)&lt;/span&gt; is of the order &lt;span class=&quot;math&quot;&gt;\(1/\epsilon_{machine}\)&lt;/span&gt;, then roundoff errors could be amplified so much that they destroy the accuracy of the computation. More specifically, we should be concerned if &lt;span class=&quot;math&quot;&gt;\(|\theta(z)|\)&lt;/span&gt; is of the order &lt;span class=&quot;math&quot;&gt;\(tol/\epsilon_{machine}\)&lt;/span&gt; where &lt;span class=&quot;math&quot;&gt;\(tol\)&lt;/span&gt; is our desired error tolerance. Of course, we only care about values of &lt;span class=&quot;math&quot;&gt;\(z\)&lt;/span&gt; that lie inside the absolute stability region &lt;span class=&quot;math&quot;&gt;\(S\)&lt;/span&gt;, since internal stability won’t matter if the computation is not absolutely stable.&lt;/p&gt;
&lt;p&gt;We can get some idea about the amplification of stage errors by plotting the curves &lt;span class=&quot;math&quot;&gt;\(|\theta(z)|=1\)&lt;/span&gt; along with the stability region. Ideally these curves will all lie outside the stability region, so that all stage errors are damped.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_02.png&quot; /&gt;
&lt;/figure&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_03.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;For both methods, we see that some of the curves intersect the absolute stability region, so some stage errors are amplified. But by how much? We’d really like to know the maximum amplification of the stage errors under the condition of absolute stability. We therefore define the &lt;em&gt;maximum internal amplification factor&lt;/em&gt; &lt;span class=&quot;math&quot;&gt;\(M\)&lt;/span&gt;:&lt;/p&gt;
&lt;center&gt;
&lt;span class=&quot;math&quot;&gt;\(M = \max_j \max_{z \in S} |\theta_j(z)|\)&lt;/span&gt;
&lt;/center&gt;

&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ssprk4&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;2.15239281554
4.04399941143&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We see that both methods have small internal amplification factors, so internal stability is not a concern in either case. This is not surprising for the method with only four stages; it is a surprisingly good property of the method with ten stages.&lt;/p&gt;
&lt;p&gt;Questions: &lt;em&gt;Do SSP RK methods always (necessarily) have small amplification factors? Can we prove it?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Now let’s look at some methods with many stages.&lt;/p&gt;
&lt;h2 id=&quot;runge-kutta-chebyshev-methods&quot;&gt;Runge-Kutta Chebyshev methods&lt;/h2&gt;
&lt;p&gt;The paper of Verwer, Hundsdorfer, and Sommeijer deals with RKC methods, which can have very many stages. The construction of these methods is implemented in NodePy, so let’s take a look at them. The functions &lt;code&gt;RKC1(s)&lt;/code&gt; and &lt;code&gt;RKC2(s)&lt;/code&gt; construct RKC methods of order 1 and 2, respectively, with &lt;span class=&quot;math&quot;&gt;\(s\)&lt;/span&gt; stages.&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;RKC41

 0    |
 1/16 |  1/16
 1/4  |  1/8   1/8
 9/16 |  3/16  1/4   1/8
______|________________________
      |   1/4   3/8   1/4   1/8&lt;/code&gt;&lt;/pre&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_04.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;It looks like there could be some significant internal amplification here. Let’s see:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    11.760869405962685
&lt;/pre&gt;


&lt;p&gt;Nothing catastrophic. Let’s try a larger value of &lt;span class=&quot;math&quot;&gt;\(s\)&lt;/span&gt;:&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    42.665327220219126
&lt;/pre&gt;


&lt;p&gt;As promised, these methods seem to have good internal stability properties. What about the second-order methods?&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RKC2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;rkc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    106.69110992619214
&lt;/pre&gt;


&lt;p&gt;Again, nothing catastrophic. We could take &lt;span class=&quot;math&quot;&gt;\(s\)&lt;/span&gt; much larger than 20, but the calculations get to be rather slow (in Python) and since we’re using floating point arithmetic, the accuracy deteriorates.&lt;/p&gt;
&lt;p&gt;Remark: &lt;em&gt;we could do the calculations in exact arithmetic using Sympy, but things would get even slower. Perhaps there are some optimizations that could be done to speed this up. Or perhaps we should use Mathematica if we need to do this kind of thing.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Remark 2: &lt;em&gt;of course, for the RKC methods the internal stability polynomials are shifted Chebyshev polynomials. So we could evaluate them directly in a stable manner using the three-term recurrence (or perhaps scipy’s special functions library). This would also be a nice check on the calculations above.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;other-methods-with-many-stages&quot;&gt;Other methods with many stages&lt;/h2&gt;
&lt;p&gt;Three other classes of methods with many stages have been implemented in NodePy:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSP families&lt;/li&gt;
&lt;li&gt;Integral deferred correction (IDC) methods&lt;/li&gt;
&lt;li&gt;Extrapolation methods&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;ssp-families&quot;&gt;SSP Families&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSPRK2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    2.0212921484995547
&lt;/pre&gt;


&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_05.png&quot; /&gt;
&lt;/figure&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;25&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# # of stages&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SSPRK3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ssprk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    3.8049237837215397
&lt;/pre&gt;


&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_06.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;The SSP methods seem to have excellent internal stability properties.&lt;/p&gt;
&lt;h3 id=&quot;idc-methods&quot;&gt;IDC methods&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DC&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;idc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;26&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;
    6.4140166271998815
&lt;/pre&gt;


&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_07.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;IDC methods also seem to have excellent internal stability.&lt;/p&gt;
&lt;h3 id=&quot;extrapolation-methods&quot;&gt;Extrapolation methods&lt;/h3&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;6&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;internal_stability_plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;16
6&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_08.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;Not so good. Let’s try a method with even more stages (this next computation will take a while; go stretch your legs).&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#order&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;46&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;
    28073.244376758907
&lt;/pre&gt;


&lt;p&gt;Now we’re starting to see something that might cause trouble, especially since such high order extrapolation methods are usually used when extremely tight error tolerances are required. Internal amplification will cause a loss of about 5 digits of accuracy here, so the best we can hope for is about 10 digits of accuracy in double precision. Higher order extrapolation methods will make things even worse. How large are their amplification factors? (Really long calculation here…)&lt;/p&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;12&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pmax&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;extrap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ex&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maximum_internal_amplification&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;1 1.99777378912
2 2.40329384375
3
 5.07204078733
4
 17.747335803
5
 69.62805786
6
 97.6097450835
7
 346.277441462
8
 1467.40356089
9
 6344.16303534
10
 28073.2443768
11
 126011.586473
12
 169897.662582&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;
    [&lt;matplotlib.lines.Line2D at 0x2611bbe10&gt;]
&lt;/pre&gt;


&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_09.png&quot; /&gt;
&lt;/figure&gt;
&lt;div class=&quot;highlight&quot;&gt;
&lt;pre&gt;&lt;span class=&quot;n&quot;&gt;semilogy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ampfac&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;linewidth&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;
&lt;pre&gt;
    [&lt;matplotlib.lines.Line2D at 0x2611a6710&gt;]
&lt;/pre&gt;


&lt;figure&gt;
&lt;img src=&quot;https://dl.dropbox.com/u/656693/jekyll_images/Internal_stability_files/Internal_stability_fig_10.png&quot; /&gt;
&lt;/figure&gt;
&lt;p&gt;We see roughly geometric growth of the internal amplification factor as a function of the order &lt;span class=&quot;math&quot;&gt;\(p\)&lt;/span&gt;. It seems clear that very high order extrapolation methods applied to problems with high accuracy requirements will fall victim to internal stability issues.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 
 <entry>
   <title>A curious upwind implicit scheme for advection</title>
   <link href="/2012/10/11/A_curious_upwind_implicit_scheme_for_advection.html"/>
   <updated>2012-10-11T00:00:00+00:00</updated>
   <id>h/2012/10/11/A_curious_upwind_implicit_scheme_for_advection</id>
   <content type="html">&lt;p&gt;Note: This post was originally published on the KAUST Mathwiki &lt;a href=&quot;https://mathwiki.kaust.edu.sa/david/A%20curious%20upwind%20implicit%20scheme%20for%20advection&quot;&gt;here&lt;/a&gt; (login required) by &lt;a href=&quot;http://www.davidketcheson.info/2012/10/11/A_curious_upwind_implicit_scheme_for_advection.html&quot;&gt;David Ketcheson&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&quot;the-cfl-condition&quot;&gt;The CFL condition&lt;/h2&gt;
&lt;p&gt;The CFL condition is one of the most basic and intuitive principles in the numerical solution of hyperbolic PDEs. First formulated by Courant, Friedrichs and Lewy in their seminal paper (in English for free &lt;a href=&quot;http://www.stat.uchicago.edu/~lekheng/courses/302/classics/courant-friedrichs-lewy.pdf&quot;&gt;here&lt;/a&gt;), it states that the domain of dependence of a numerical method for solving a PDE must contain the true domain of dependence. Otherwise, the numerical method cannot be convergent.&lt;/p&gt;
&lt;p&gt;The CFL condition is geometric and easily understood in the context of, say, a first-order upwind discretization of advection. Usually it says nothing interesting about implicit schemes, since they include all points in their domain of dependence. But sometimes understanding the CFL condition for a particular scheme can be subtle.&lt;/p&gt;
&lt;h3 id=&quot;an-implicit-scheme&quot;&gt;An implicit scheme&lt;/h3&gt;
&lt;p&gt;Consider the advection equation&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[u_t + a u_x = 0.\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Discretization using a backward difference in space and in time gives the scheme&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\[U^{n+1}_j = U^n_j - \nu(U^{n+1}_j - U^{n+1}_{j-1}).\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where &lt;span class=&quot;math&quot;&gt;\(\nu = ka/h\)&lt;/span&gt; is the CFL number and &lt;span class=&quot;math&quot;&gt;\(k,h\)&lt;/span&gt; are the step sizes in time and space, respectively. This very simple scheme illustrates the concepts of the CFL condition and stability in a remarkable way.&lt;/p&gt;
&lt;p&gt;For simplicity, suppose that the problem is posed on the domain &lt;span class=&quot;math&quot;&gt;\(0\le x \le 1\)&lt;/span&gt;, with an appropriate boundary condition. Since this scheme computes &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_j\)&lt;/span&gt; in terms of &lt;span class=&quot;math&quot;&gt;\(U^n_j\)&lt;/span&gt; and &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_{j-1}\)&lt;/span&gt;, it seems that the numerical domain of dependence for &lt;span class=&quot;math&quot;&gt;\(U^n_j\)&lt;/span&gt; is &lt;span class=&quot;math&quot;&gt;\((x,t)\in (0,x_j)\times[0,t_n]\)&lt;/span&gt;. Based on this, we may conclude that the scheme is not convergent for &lt;span class=&quot;math&quot;&gt;\(\nu&amp;lt;0\)&lt;/span&gt;. Simple enough.&lt;/p&gt;
&lt;p&gt;But what if &lt;span class=&quot;math&quot;&gt;\(\nu=-1\)&lt;/span&gt;? Then the scheme reads &lt;span class=&quot;math&quot;&gt;\[U^{n+1}_{j-1} = U^n_j,\]&lt;/span&gt; which gives &lt;strong&gt;the exact solution&lt;/strong&gt;! This is a sort of “anti-unit CFL condition”.&lt;/p&gt;
&lt;p&gt;How can this scheme be convergent (in fact, exact!) for a negative CFL number when it doesn’t use any values to the right?&lt;/p&gt;
&lt;h3 id=&quot;understanding-the-cfl-condition&quot;&gt;Understanding the CFL condition&lt;/h3&gt;
&lt;p&gt;Look at the exact formula above. In this case the scheme is not a method for computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_j\)&lt;/span&gt; but for computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_{j-1}\)&lt;/span&gt;, and it &lt;em&gt;does&lt;/em&gt; use a value from the previous time step that lies to the right.&lt;/p&gt;
&lt;p&gt;So we can view the scheme with &lt;span class=&quot;math&quot;&gt;\(\nu=-1\)&lt;/span&gt; as a method for computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_j\)&lt;/span&gt;, in which case the CFL condition is satisfied only for &lt;span class=&quot;math&quot;&gt;\(\nu\ge0\)&lt;/span&gt;, or we can view the scheme as a method for computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_{j-1}\)&lt;/span&gt;, in which case the CFL condition is satisfied only for &lt;span class=&quot;math&quot;&gt;\(\nu\le-1\)&lt;/span&gt;. &lt;strong&gt;Which viewpoint is correct?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To answer that question, remember that the CFL condition is purely algebraic – that is, it relates to which values are actually used to compute which other values. To understand this scheme, we need to think about how we actually solve for &lt;span class=&quot;math&quot;&gt;\(U^{n+1}\)&lt;/span&gt; when using it. Notice that the scheme can be written as &lt;span class=&quot;math&quot;&gt;\[A U^{n+1} = U^n\]&lt;/span&gt; where the matrix &lt;span class=&quot;math&quot;&gt;\(A\)&lt;/span&gt; is lower-triangular. Hence the system can be solved by substitution. To go further, we must consider two cases:&lt;/p&gt;
&lt;ol type=&quot;1&quot;&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\(\nu&amp;gt;0\)&lt;/span&gt;: in this case, boundary values must be supplied along the left boundary at &lt;span class=&quot;math&quot;&gt;\(x=0\)&lt;/span&gt;. Then, starting from the known value at the boundary, we work to the right by substitution: &lt;span class=&quot;math&quot;&gt;\[U^{n+1}_j = \frac{U^n_j+\nu U^{n+1}_{j-1}}{1+\nu}.\]&lt;/span&gt; Hence the scheme is truly a way of computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_j\)&lt;/span&gt; based on &lt;span class=&quot;math&quot;&gt;\(U^n_j, U^{n+1}_{j-1}\)&lt;/span&gt; and the resulting CFL condition is &lt;span class=&quot;math&quot;&gt;\(\nu\ge0\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;span class=&quot;math&quot;&gt;\(\nu&amp;lt;0\)&lt;/span&gt;: in this case, boundary values must be supplied along the right boundary at &lt;span class=&quot;math&quot;&gt;\(x=1\)&lt;/span&gt;. Then, starting from the known value at the boundary, we work to the left by substitution: &lt;span class=&quot;math&quot;&gt;\[U^{n+1}_{j-1} = \frac{(1+\nu)U^{n+1}_j - U^n_j}{\nu}.\]&lt;/span&gt; Hence the scheme is truly a way of computing &lt;span class=&quot;math&quot;&gt;\(U^{n+1}_{j-1}\)&lt;/span&gt; based on &lt;span class=&quot;math&quot;&gt;\(U^n_j, U^{n+1}_{j}\)&lt;/span&gt; and the resulting CFL condition is &lt;span class=&quot;math&quot;&gt;\(\nu\le-1\)&lt;/span&gt;.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&quot;references&quot;&gt;References:&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Courant%E2%80%93Friedrichs%E2%80%93Lewy_condition&quot;&gt;CFL condition&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://http://dx.doi.org/10.1007%2FBF01448839&quot;&gt;their seminal paper&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;http://www.stanford.edu/class/cme324/classics/courant-friedrichs-lewy.pdf&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;div class=&quot;references&quot;&gt;

&lt;/div&gt;
</content>
 </entry>
 

</feed>
